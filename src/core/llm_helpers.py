"""
LLM Integration Helpers - Funções especializadas para integrações LLM no AiShortsV2.
Centraliza contratos e padrões de chamada ao AsyncOpenRouterClient.
"""

import logging
from typing import Dict, Any, List, Optional, Literal
from dataclasses import dataclass
from pydantic import BaseModel, Field
from datetime import datetime

from src.core.async_openrouter_client import get_async_openrouter_client
from src.models.unified_models import GeneratedTheme, GeneratedScript
from src.validators.script_validator import ValidationReport
from src.core.content_cache import get_content_cache, cached_response

logger = logging.getLogger(__name__)

# Feature flags (podem vir do settings.py)
USE_LLM_THEME_STRATEGY = True
USE_LLM_SCRIPT_REFINER = True
USE_LLM_BROLL_PLANNER = True
USE_LLM_RERANKER = False
USE_LLM_CO_REVIEWER = False
USE_LLM_CAPTION_VALIDATOR = False


# Modelos de resposta para validação
class ThemeStrategyResult(BaseModel):
    """Resultado do Theme Strategy Engine LLM."""
    topic: str = Field(description="Texto curto do tema em formato hook")
    angle: str = Field(description="Explicação de 1 frase do diferencial")
    safety_flags: List[str] = Field(default_factory=list)
    uniqueness_score: float = Field(ge=0.0, le=1.0)
    virality_potential: float = Field(ge=0.0, le=1.0)


class ScriptRefinerRequest(BaseModel):
    """Requisição para refinar script."""
    platform: Literal["tiktok", "shorts", "reels"]
    theme: str
    previous_script: Dict[str, Any]
    validation_summary: Dict[str, Any]
    constraints: Dict[str, Any]


class ScriptRefinerResult(BaseModel):
    """Resultado do refino de script."""
    hook: str
    body: str
    conclusion: str
    estimated_duration: int
    refinement_notes: List[str] = Field(default_factory=list)


class BrollPlannerRequest(BaseModel):
    """Requisição para planejamento de B-roll."""
    script_text: str
    max_queries: int = 6
    visual_roles: List[str] = Field(
        default_factory=lambda: [
            "establishing_shot", 
            "subject_closeup", 
            "dynamic_motion", 
            "emotional_reaction"
        ]
    )
    guidelines: List[str] = Field(
        default_factory=lambda: [
            "use concrete visual nouns",
            "avoid generic queries",
            "prefer 3-6 words per query"
        ]
    )


class BrollQueryItem(BaseModel):
    """Item de query B-roll."""
    text: str = Field(description="Query do YouTube")
    role: str = Field(description="Papel visual")
    priority: float = Field(ge=0.0, le=1.0)


class BrollPlannerResult(BaseModel):
    """Resultado do planejamento de B-roll."""
    queries: List[BrollQueryItem]


class CandidateInfo(BaseModel):
    """Informações do candidato para reranking."""
    id: str
    title: str
    description: str
    clip_score: float
    view_count: int
    duration: float


class RerankRequest(BaseModel):
    """Requisição para reranking."""
    script_summary: str
    candidates: List[CandidateInfo]


class RerankScore(BaseModel):
    """Score de reranking."""
    id: str
    llm_relevance: float = Field(ge=0.0, le=1.0)
    reason: str
    final_score: Optional[float] = None


class RerankResult(BaseModel):
    """Resultado do reranking."""
    scores: List[RerankScore]


class LLMHelpers:
    """Helper functions para integrações LLM."""
    
    def __init__(self):
        self.client = get_async_openrouter_client()
        self.cache = get_content_cache()  # Cache inteligente
    
    @cached_response(ttl_hours=48)  # Temas podem ser reusados por 2 dias
    async def generate_theme_strategy(
        self,
        category: str,
        recent_themes: List[str],
        constraints: Optional[Dict[str, Any]] = None
    ) -> ThemeStrategyResult:
        """
        Gera tema otimizado via LLM.
        
        Args:
            category: Categoria do tema
            recent_themes: Lista de temas recentes usados
            constraints: Constraints específicas
        
        Returns:
            ThemeStrategyResult com tema anotado
        """
        if not USE_LLM_THEME_STRATEGY:
            # Fallback para tema aleatório
            from src.generators.theme_generator import ThemeGenerator
            generator = ThemeGenerator()
            theme = generator.generate_single_theme(category)
            return ThemeStrategyResult(
                topic=theme.content,
                angle="Generated by traditional method",
                uniqueness_score=0.5,
                virality_potential=0.6
            )
        
        system_message = """You are the Theme Strategy Engine for an AI short-form video factory. 
        Output ONE topic optimized for virality, novelty, and factual robustness, in JSON only."""
        
        payload = {
            "category": category,
            "recent_themes": recent_themes[-10:],  # Últimos 10 temas
            "language": "pt-BR",
            "constraints": constraints or {
                "max_words": 40,
                "must_be_fact_based": True,
                "avoid_overlap_with_recent": True,
                "avoid_generic": True
            }
        }
        
        try:
            result = await self.client.generate_json(
                system_message=system_message,
                user_message=str(payload),
                temperature=0.7,
                max_tokens=256
            )
            
            # Validar e converter
            validated = ThemeStrategyResult(**result)
logger.info(f" Tema LLM gerado: {validated.topic[:30]}...")
logger.info(f"   Uniqueness: {validated.uniqueness_score:.2f}, Virality: {validated.virality_potential:.2f}")
            
            return validated
            
        except Exception as e:
logger.error(f" Erro ao gerar tema via LLM: {e}")
            raise
    
    async def refine_script(
        self,
        platform: Literal["tiktok", "shorts", "reels"],
        theme: str,
        previous_script: Dict[str, Any],
        validation_summary: Dict[str, Any],
        constraints: Optional[Dict[str, Any]] = None
    ) -> ScriptRefinerResult:
        """
        Refina script baseado no ValidationReport usando LLM.
        
        Args:
            platform: Plataforma alvo
            theme: Tema do script
            previous_script: Script anterior
            validation_summary: Resumo da validação
            constraints: Constraints do script
        
        Returns:
            ScriptRefinerResult com script refinado
        """
        if not USE_LLM_SCRIPT_REFINER:
            raise ValueError("Script refiner LLM não está ativado")
        
        system_message = """You are a short-form script rewriting engine. 
        Use the validation report to fix problems without breaking constraints. 
        Output in a strict labeled format."""
        
        payload = ScriptRefinerRequest(
            platform=platform,
            theme=theme,
            previous_script=previous_script,
            validation_summary=validation_summary,
            constraints=constraints or {
                "sections": ["HOOK", "BODY", "CONCLUSION"],
                "target_duration_seconds": [50, 65],
                "language": "pt-BR",
                "safe_for_ads": True
            }
        )
        
        try:
            result = await self.client.generate_json(
                system_message=system_message,
                user_message=payload.model_dump_json(),
                temperature=0.6,
                max_tokens=700
            )
            
            # Se for JSON estruturado, usar diretamente
            if isinstance(result, dict):
                return ScriptRefinerResult(
                    hook=result.get("hook", ""),
                    body=result.get("body", ""),
                    conclusion=result.get("conclusion", ""),
                    estimated_duration=result.get("estimated_duration", 60),
                    refinement_notes=result.get("refinement_notes", [])
                )
            
            # Se for texto, processar como antes
            lines = str(result).strip().split('\n')
            script_dict = {}
            current_section = None
            current_text = []
            
            for line in lines:
                line = line.strip()
                if line in ["HOOK:", "BODY:", "CONCLUSION:", "ESTIMATED_DURATION:"]:
                    if current_section and current_text:
                        script_dict[current_section] = ' '.join(current_text).strip()
                    current_section = line.replace(":", "")
                    current_text = []
                    if line == "ESTIMATED_DURATION:":
                        # Próxima linha terá a duração
                        continue
                elif current_section and line:
                    current_text.append(line)
            
            # Última seção
            if current_section and current_text:
                script_dict[current_section] = ' '.join(current_text).strip()
            
            validated = ScriptRefinerResult(
                hook=script_dict.get("HOOK", ""),
                body=script_dict.get("BODY", ""),
                conclusion=script_dict.get("CONCLUSION", ""),
                estimated_duration=int(script_dict.get("ESTIMATED_DURATION", 60)),
                refinement_notes=[]
            )
            
logger.info(f" Script refinado via LLM para {platform}")
            return validated
            
        except Exception as e:
logger.error(f" Erro ao refinar script via LLM: {e}")
            raise
    
    @cached_response(ttl_hours=168)  # Queries por 1 semana
    async def plan_broll_queries(
        self,
        script_text: str,
        visual_roles: Optional[List[str]] = None,
        max_queries: int = 6
    ) -> BrollPlannerResult:
        """
        Planeja queries de B-roll usando LLM.
        
        Args:
            script_text: Texto do roteiro
            visual_roles: Papéis visuais desejados
            max_queries: Número máximo de queries
        
        Returns:
            BrollPlannerResult com queries priorizadas
        """
        if not USE_LLM_BROLL_PLANNER:
            # Fallback para método atual
            from src.video.matching.semantic_analyzer import SemanticAnalyzer
            analyzer = SemanticAnalyzer()
            keywords = analyzer.extract_keywords(script_text)
            queries = [f"{' '.join(keywords[:3])}" if keywords else script_text[:40]]
            return BrollPlannerResult(
                queries=[BrollQueryItem(text=q, role="general", priority=0.5) for q in queries[:max_queries]]
            )
        
        system_message = """You are a B-roll query planner. 
        Given the script, output highly specific YouTube search queries 
        that cover different visual roles. JSON only."""
        
        payload = BrollPlannerRequest(
            script_text=script_text[:500],  # Limitar para performance
            visual_roles=visual_roles or [
                "establishing_shot",
                "subject_closeup", 
                "dynamic_motion",
                "emotional_reaction"
            ],
            max_queries=max_queries,
            guidelines=[
                "use concrete visual nouns",
                "avoid generic queries",
                "prefer 3-6 words per query"
            ]
        )
        
        try:
            result = await self.client.generate_json(
                system_message=system_message,
                user_message=payload.model_dump_json(),
                temperature=0.8,
                max_tokens=512
            )
            
            # Converter para BrollPlannerResult
            validated = BrollPlannerResult.parse_obj(result)
logger.info(f" Planejadas {len(validated.queries)} queries de B-roll")
            
            return validated
            
        except Exception as e:
logger.error(f" Erro ao planejar B-roll via LLM: {e}")
            raise
    
    async def rerank_candidates(
        self,
        script_summary: str,
        candidates: List[Dict[str, Any]],
        weights: Optional[Dict[str, float]] = None
    ) -> RerankResult:
        """
        Rerank candidatos usando compreensão textual LLM.
        
        Args:
            script_summary: Resumo do roteiro
            candidates: Lista de candidatos CLIP
            weights: Pesos para combinar scores
        
        Returns:
            RerankResult com scores LLM
        """
        if not USE_LLM_RERANKER:
            # Retornar scores CLIP originais
            scores = [
                RerankScore(
                    id=c.get("id", ""),
                    llm_relevance=c.get("relevance_score", c.get("clip_score", 0.5)),
                    reason="CLIP score"
                )
                for c in candidates
            ]
            return RerankResult(scores=scores)
        
        system_message = """You are a ranking engine. 
        Score how well each candidate clip visually supports the script's narrative. 
        Respond JSON only."""
        
        # Preparar candidatos
        candidate_infos = []
        for c in candidates[:10]:  # Limitar para performance
            candidate_infos.append(CandidateInfo(
                id=c.get("id", ""),
                title=c.get("title", ""),
                description=c.get("description", ""),
                clip_score=c.get("relevance_score", c.get("clip_score", 0.5)),
                view_count=c.get("view_count", 0),
                duration=c.get("duration", 0.0)
            ))
        
        payload = RerankRequest(
            script_summary=script_summary[:200],
            candidates=[c.model_dump() for c in candidate_infos]
        )
        
        try:
            result = await self.client.generate_json(
                system_message=system_message,
                user_message=payload.model_dump_json(),
                temperature=0.3,
                max_tokens=1024
            )
            
            validated = RerankResult.parse_obj(result)
            
            # Combinar scores CLIP e LLM
            default_weights = {"clip": 0.6, "llm": 0.4}
            final_weights = {**default_weights, **(weights or {})}
            
            scores = []
            for i, c in enumerate(candidate_infos):
                clip_score = c.clip_score
                llm_score = validated.scores[i].llm_relevance if i < len(validated.scores) else 0.5
                final_score = final_weights.get("clip", 0.6) * clip_score + final_weights.get("llm", 0.4) * llm_score
                
                scores.append(RerankScore(
                    id=c.id,
                    llm_relevance=llm_score,
                    reason=validated.scores[i].reason if i < len(validated.scores) else "No LLM score",
                    final_score=final_score
                ))
            
logger.info(f" Reranked {len(scores)} candidatos com LLM")
            return RerankResult(scores=scores)
            
        except Exception as e:
logger.error(f" Erro ao rerank via LLM: {e}")
            raise
    
    async def validate_script_with_llm(
        self,
        script: Dict[str, Any],
        platform: str,
        theme: str,
        rules: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Valida script usando LLM como co-revisor.
        
        Args:
            script: Script a validar
            platform: Plataforma alvo
            theme: Tema do script
            rules: Regras específicas
        
        Returns:
            Relatório de validação complementar
        """
        if not USE_LLM_CO_REVIEWER:
            return {}
        
        system_message = """You are a strict reviewer for 60-second short-form scripts. 
        Check coherence, safety, hype vs. facts, and engagement. Output JSON only."""
        
        payload = {
            "platform": platform,
            "theme": theme,
            "script": script,
            "rules": rules or {
                "max_duration": 70,
                "banned_words": ["clickbait", "buy now", "subscribe"],
                "required_style": "educational, precise",
                "language": "pt-BR"
            }
        }
        
        try:
            result = await self.client.generate_json(
                system_message=system_message,
                user_message=str(payload),
                temperature=0.2,
                max_tokens=512
            )
            
            validated = result  # Resultado já é dict
            
            # Adicionar prefixo LLM_ aos campos para não conflitar
            llm_report = {}
            for key, value in validated.items():
                if key == "issues":
                    llm_report["LLM_" + key] = [
                        {
                            **issue,
                            "origin": "llm"
                        }
                        for issue in value
                    ]
                else:
                    llm_report["LLM_" + key] = value
            
logger.info(" Validação LLM concluída")
            return llm_report
            
        except Exception as e:
logger.error(f" Erro na validação LLM: {e}")
            return {}
    
    async def validate_captions(
        self,
        script_text: str,
        captions: List[Dict[str, Any]],
        constraints: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Valida legendas usando LLM.
        
        Args:
            script_text: Texto original do script
            captions: Legendas geradas
            constraints: Constraints de estilo
        
        Returns:
            Relatório de validação
        """
        if not USE_LLM_CAPTION_VALIDATOR:
            return {"is_acceptable": True}
        
        system_message = """You verify if captions accurately represent the script 
        and follow the style constraints. Output JSON only."""
        
        payload = {
            "script_text": script_text,
            "captions": captions,
            "constraints": constraints or {
                "max_chars_per_line": 42,
                "max_lines": 2
            }
        }
        
        try:
            result = await self.client.generate_json(
                system_message=system_message,
                user_message=str(payload),
                temperature=0.1,
                max_tokens=256
            )
            
logger.info(f" Validação de legendas: {result.get('is_acceptable', False)}")
            return result
            
        except Exception as e:
logger.error(f" Erro ao validar legendas: {e}")
            return {"is_acceptable": True, "error": str(e)}