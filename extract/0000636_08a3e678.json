{"origin_pdf_path": "https://eurasip.org/Proceedings/Eusipco/Eusipco2024/pdfs/0000636.pdf", "text_in_pdf": "Scoring synchronization between music and motion: local Vs global approaches  \n\nHamza BAYD, Patrice GUYOT, Benoit BARDY, Pierre SLANGEN EuroMov Digital Health in Motion, Univ. Montpellier, IMT Mines Ales, Ales, France  \n\nAbstract—This paper compares methods for scoring the synchronization between music and motion. A wide range of local and global methods such as the Gaussian-based method, relative phase, and dynamic time warping (DTW) have been evaluated. Firstly, these methods were tested on synthetic data representing various scenarios of motion and music interaction. Secondly, we introduce a full system able to evaluate real video performances of music and movement synchronization in which participants were asked to tap their fingers to the beat. The system takes a motion video clip as input and extracts music and motion beats. From these two vectors, the synchronization is scored with different metrics. Results emphasize the complementary nature of the scoring approaches. The data and code are available online at https://github.com/baya1235/Scoring-synchronizationbetween-motion-and-music.  \n\nIndex Terms—Synchronization, Music information retrieval, Motion capture  \n\nII. RELATED WORK  \n\nA. Musical rhythmic features extraction  \n\nThe rhythm in music is generally characterized by a sequence of beats, downbeats, and other patterns. These rhythmic features are perceived by listeners [1] and can be automatically detected through the scope of Music Information Retrieval (MIR). Traditional approaches for rhythm analysis rely on two components. First, low-level features are extracted from the audio signal, and then the features are analyzed to retrieve the requested rhythmic information such as tempo, onset and beat. These steps are implemented in different python frameworks such as Essentia [2], and Librosa [3]. More recent approaches merge the two steps with with neural networks, such as Madmom [4].  \n\nI. INTRODUCTION  \n\nPeople move when listening to music, and through movement, they give meaning to the music. In the dynamic intersection of movement and music, the assessment of synchronization scoring plays a pivotal role in quantifying the alignment and harmony between the temporal events of successive groups of rhythmical bodily motions and the rhythmic elements present in musical compositions. While synchronization is often intuitively perceived in performance arts such as dance, automating and objectifying this evaluation presents a significant challenge, requiring technological finesse and a profound understanding of the subtleties of human movement and music. The connection between how we perceive music and our ability to synchronize our movements with regulated timing is facilitated by the inherent predictability found in musical expectations. This paper focuses on various methods and contributions aimed at improving the precision of scoring synchronization between motion and music beats. The paper is organized as follows: Section 2 introduces related work and Section 3 details the approaches for scoring synchronization and their application with synthetic data. Here, the initial step involves the generation of synthetic data representing various scenarios of motion and music interaction. This diverse dataset serves as the foundation for evaluating the effectiveness of synchronization scoring methods under different conditions. Experimental results are presented in Section 4, where we use real audio-video data to evaluate synchronization as well as a method of motion beat extraction from movements, and Section 5 concludes the paper.  \n\nB. Motion beat extraction  \n\nResearch on motion analysis primarily focuses on three main factors: motion magnitude (or velocity), motion direction, and motion trajectory [5]. Motion beat refers to the regular moment when the movement changes significantly during a dance performance [6]. Quantitative movement analyses often require costly devices such as 3D marker-based systems, accelerometers, and gyroscopes [7]. During the last few years, significant progress has been made in challenging computer vision problems, including human pose estimation and motion capture, in particular using neural networks such as OpenPose and MediaPipe [8] which are less expensive and less cumbersome alternatives.  \n\nC. Synchronization evaluation  \n\nMotion sciences and MIR are two key domains engaged in synchronization evaluation research. Each of these fields adopts its approaches and perspectives. Even with all the advancements in understanding body movement and rhythm through data, the methods for automatically scoring the connection between motion and music still have considerable ambiguity and haven’t been thoroughly researched. There are two main ways to evaluate synchronization: subjectively, taking into account different parameters such as culture, music experience, or the movement context. The main limitation arises from the fact that many pieces of music can be perceived as synchronized with the same dance movements, or conversely, the same piece of music may be synchronized with different dance styles.  \n\n  \nFig. 1. Notations for references and observations  \n\nThe second way is to evaluate synchronization objectively, through the comparison of beat annotations with a sequence of motion, and our article follows this path. Early work in this field has focused primarily on binary methods [5]. More recent research has focused on phase synchrony analysis [10], or has proposed to measure synchronization globally, based on tempo and inter-beat estimation [9]. Each of these approaches has its advantages and limitations, and may not be entirely robust across all scenarios. Additionally, these approaches do not provide a single score that directly represents synchronization performance, but rather indicators that must be interpreted to assess the degree of synchronization.  \n\nIII. METHODS  \n\nWe adopt a standardized notation illustrated in Figure 1. The symbol $r_{i}$ represent the i-th reference value (music beat). For each reference $r_{i}$ we consider the closest observation $o_{j}$ (motion beat) ; other observations are considered as added. We consider missed observations when there is no observation between two references.  \n\nWe note $\\Delta_{r}$ the inter-reference interval, which is approximately constant throughout the musical piece although small differences may occur. Given a sequence of references $r$ of lengths $N$ and a sequence of observations of length $M$ , we calculate the time difference between a reference $r_{i}$ and its closest observation $o_{i}$ . We note $x_{i}$ is the absolute time delay between them.  \n\n3) Sigmoid-based method: employs a sigmoid function for weighting the temporal phase differences between observation and reference events. The fundamental contribution lies in assigning weights to these temporal phases, with smaller phase differences resulting in higher synchronization scores. Essentially, the sigmoid function introduces a non-linear weighting mechanism, enhancing the evaluation process.  \n\nThe synchronization score between two vectors’ reference and observation is defined as follows:  \n\n$$\nL M_{S i g}(i)=2\\left(1-\\frac{1}{1+e^{-c x}}\\right)\n$$  \n\nwhere $c$ is a parameter controlling the slope of the sigmo¨ıd and the sensitivity of the synchronization score, larger value resulting in a steeper slope, making the evaluation score more stringent and challenging. This produces a synchronization score that assigns a value of 1 when reference and observation are perfectly synchronized, which decreases to 0 as de-synchronization increases.  \n\nB. Global Measurement  \n\n1) Fscore: from a binary local measurement, we compute the F-score, a widely adopted metric for assessing beat/downbeat tracking performance [11]. In this study, we extend this methodology to evaluate temporal synchronization.  \n\n$$\nF s c o r e=\\frac{2t p}{2t p+f^{+}+f^{-}}\n$$  \n\nwhere $t p$ represents the number of correct synchronizations (true positives), $f^{+}$ indicates the false positives (added) and $f^{-}$ denotes the number of false negatives (missed).  \n\n2) Mean: from each computed local score, we compute the mean synchronization score.  \n\n$$\nx_{i}=\\operatorname{min}_{j}\\lVert\\boldsymbol{r}_{i}-o_{j}\\rVert\n$$  \n\nA. Local Measurement  \n\n1) Binary measure with threshold: uses a tolerance window $\\theta_{t}$ around the reference. Within this ”top-hat” tolerance window, precisely matching observations are identified as good synchronization. Any additional observation detected within each tolerance window, or outside any tolerance window, is considered as added. Empty tolerance windows with no corresponding reference were considered missed.  \n\n2) Gaussian-based method: this alternative approach involves employing Gaussian distributions centered around each reference point. This technique is inspired from method used in (MIR) evaluation [11]. It provides a continuous measure of synchronization located at the level of the nearest annotation.  \n\n$$\nL M_{G a u}(i)=\\exp\\left(\\frac{-x^{2}}{2\\sigma^{2}}\\right)\n$$  \n\nwhere $\\mathbf{X}$ is the absolute time delay between observation and reference and $\\sigma$ the standard deviation. It provides an accuracy score over the continuous range from 0 to 1 rather than a binary decision of a correct or incorrect observation.  \n\n$$\nM e a n=\\frac{1}{\\operatorname{max}(N,M)}\\sum_{i=1}^{N}L M(i)\n$$  \n\nAdded and missed observations are explicitly accounted for by normalizing the denominator, where the mean score will linearly decrease as the number of observation $M$ exceeds the number of references $N$ . This calculation produces an average synchronization score across all pairs.  \n\n3) Dynamic Time Warping: in contrast, the Dynamic Time Warping (DTW) algorithm finds the optimal alignment between two complete sequences [12] (including missed and added) by minimizing the total distance between corresponding points. The DTW distance $\\mathrm{D}(\\mathbf{r},\\,\\mathrm{o})$ is calculated using the following recurrence relation. $\\forall i\\in\\{2,N\\}$ and $\\forall j\\in\\{2,M\\}$ :  \n\n$$\nD(r,o)=+m i n\\left\\{{\\cal D}(i-1,j)\\atop{\\cal D}(i,j-1)\\right\\}\n$$  \n\nUsing the min-max normalization, we normalize the DTW distance to convert it into a standardized score between 0 and 1. The DTW score is calculated using the formula :  \n\n$$\ns c o r e(D T W)=1-\\frac{D-\\mathrm{min_{D}}}{\\mathrm{max_{D}-m i n_{D}}}\n$$  \n\nwhere minD and maxD are the minimum and maximum DTW distances observed in the sequences.  \n\n4) Circular statistics method: relies on the relative phase $\\phi_{j}$ of the observations $o_{j}$ positioned between two references $r_{i}$ and $r_{i+1}$ .  \n\n$$\n\\phi_{j}=360\\times\\operatorname{min}_{i}\\lVert r_{i}-o_{j}\\rVert\\times\\frac{1}{r_{i+1}-r_{i}}\n$$  \n\nUsing circular statistics [13], specifically the mean direction angular phase $\\Phi$ and mean length vector $$ provides insights into the concentration and dispersion of these phases.  \n\n$$\n\\Phi=\\arg\\left(\\sum_{j=1}^{M}\\frac{e^{j\\cdot\\mathrm{radians}(\\phi_{j})}}{M}\\right)\n$$  \n\n$$\n\n$$  \n\nwhere $N$ is the total number of pairs.  \n\n$\\mathbf{R}$ can range from 0 (random/uniform distribution of phases) to 1 (steady phase). Mean direction angular phases $\\Phi$ , range between $-180^{\\circ}$ to $+180^{\\circ}$ degrees. A value of 0 degrees indicates good synchronization on average, while a value of 180 degrees indicates syncopation.  \n\n5) Continuity-based: taking cues from MIR evaluation methodologies [11], we adapted the approach to measure the continuity of synchronization. Continuity relies on a binary tolerance windows $\\theta_{r}$ calculated relatively of each inter-reference interval around each reference $r_{j}$ . The closest observation $o_{i}$ to each reference is considered as correct if it falls within the tolerance window and if the previous observation $o_{i-1}$ also falls within its tolerance window.  \n\nThe continuity conditions can be summarized as :  \n\n$$\n\\begin{array}{r l}&{r_{j}-\\theta_{r}\\Delta_{r}<\\gamma_{o}<r_{j}+\\theta_{r}\\Delta_{r}}\\\\ &{r_{j-1}-\\theta_{r}\\Delta_{r-1}<\\gamma_{o-1}<r_{j-1}+\\theta\\Delta_{r-1}}\\\\ &{(1-\\theta_{r})\\Delta_{r}<\\Delta_{o}<(1+\\theta_{r})\\Delta_{r}}\\end{array}\n$$  \n\nWhere $\\Delta_{r}$ is inter-reference-interval, and $\\Delta_{o}$ interobservation-interval. We can estimate the number of correct observations in each continuously correct segment $\\Upsilon_{n}$ , where there are $n$ continuous segments. Based on this analysis, we define the continuity-based score $C S L_{c}$ as the portion of the correct longest single continuous sync segment, and the portion of the total of continuous sync segments as $C S L_{t}$ .  \n\n$$\nC S L_{c}=\\frac{\\operatorname{max}(\\Upsilon_{n})}{J}\n$$  \n\n$$\nC S L_{t}=\\frac{\\sum_{n=1}^{N}\\Upsilon_{n}}{J}\n$$  \n\nFinally, in forthcoming experiments, the parameters for scoring methods have been fixed as follows: $\\theta_{t}\\,=\\,\\pm70\\mathrm{{ms}}$ , $\\sigma=\\Delta_{r}/2,\\,c=10,\\,\\theta_{r}=15\\%$ .  \n\nTABLE I PERFORMANCE SYNCHRONIZATION SCORE (IN PERCENT $\\%$ )   \n\nTestF-scoreGaussianSigmoidDTWRCSL(c -t)Missed10%94919193.0810038-80Missed50%655050331009-22Shift 10%083628710096 -96Shift 30%02519671000 -0Matching %069698979997100-100Matching 50%778681947726 -61Randomized19233054140-0  \n\nC. Experiment with synthesized data  \n\nTo evaluate the effectiveness of synchronization scoring methods under different conditions, we generated synthetic data representing diverse music/motion interaction scenarios. References are based on a beat tracking computed on a real music track $(\\Delta_{r}\\approx720~\\mathrm{ms})$ . The generated sequence of observations match the length of the reference vector $[N=M]$ ), except in the case of the missed scenario. Figure 2 illustrates different scenarios:  \n\n• Missed where a certain percentage of observations are missing while the others are perfectly synchronized,   \n• Shift where all observations are shifted with the same lag (set as a percentage of $\\Delta_{r}$ ),   \n• Matching where a percentage of observations are perfectly synchronized while the others are randomly shifted $(\\pm100~\\mathrm{ms})$ ,   \n• Randomized where all observations are randomly shifted around their references (shift $<\\Delta_{r}/2\\$ ).  \n\n  \nFig. 2. Synthesized data examples and notations.  \n\nFrom the reference and observations time vectors that have been generated, we calculated six synchronization scores:  \n\n• $F_{\\mathrm{{-}}}$ -score, and mean of Gaussian and Sigmo¨ıd scores relied on local independent time delay estimations, • DTW, $R$ and $C S L$ provided a broader view on the global performance.  \n\nTable 1 provides a comparative analysis of our synchronization scores applied to the synthesized scenarios. Firstly, we consider the local-based $F$ -score, Gaussian, and Sigmo¨ıd. They provide an interesting evaluation of the missed scenario. In particular, Gaussian and Sigmo¨ıd scores decrease linearly with the percentage of missed observations. As expected, $F$ -score based on a binary threshold is very impacted by shifted observations, while Gaussian and Sigmo¨ıd scores provide a more sensitive and continuous evaluation. The matching scenario has little impact on these scores. Randomized observations are slightly penalized, although it might be considered as the worst scenario.  \n\nSecondly, if we consider global approaches, DTW and CSL are very impacted by the missed observations. The $C S L$ , based on a binary threshold as $F$ -score, is also impacted by shifted observations, while $R$ does not take them into account. DTW shows a fairly high score when the number of references and observations is the same, even if it falls to $54\\%$ for random values. $R$ and $C S L$ show their interest by giving the lowest values for the randomized scenario.  \n\nIV. EXPERIMENT WITH REAL-WORLD DATA  \n\nWhile synthetic data is useful for assessing defined scenarios, real-world experiments often reveal unexpected phenomena. In this section, we asked human participants to synchronize their motions with music. We focused on finger tapping, a classical task used to investigate sensorimotor synchronization. Participants were asked to tap their thumb and forefinger in sync with the music beats as accurately as possible (in-phase synchronization, see Figure 3).  \n\nA. Camera setup and audio-video recording  \n\nMusic was played through headphones. The webcam1 was connected to a Linux PC and positioned to capture the upper body directly. The distance between the participant and the webcam was set at 3 meters. An application based on Python libraries2 was implemented to handle the recording of audiovideo playbacks while focusing on the precise alignment between the recorded motion and the accompanying music track.  \n\nB. Automatic scoring of music/motion synchronization  \n\n1) Music beat retrieval: We utilized a 30-second excerpt3 as the auditory stimulus. The excerpt was sourced from the GTZAN Rhythm database [14] with a medium tempo (110 bpm) and belongs to the disco genre. Automatic analysis, known as beat tracking, was carried out using the Madmom library [4] to identify the beats in the time series.  \n\n2) Motion beat retrieval: Hand motion is captured with MediaPipe [15], a deep learning model implemented in Python. It relies on the analyze of 21 hand-knuckle coordinates. Based on the extracted coordinates we computed the Euclidean distance between thumb and forefinger at each frame. Finally, we applied a min-max normalization to the computed distance within each video.  \n\nThen we used a peak detection algorithm to find motion beats, as illustrated in Figure 4. First, we identify time ranges for each motion beat $(s p a c i n g\\,<\\,1/2)$ , each sample of the time ranges being considered as a motion beat candidate.  \n\n  \nFig. 3. Finger tapping while listening to music. The motion beat is detected when the spacing between the forefinger and thumb (in blue) is minimum.  \n\n  \nFig. 4. Motion beat detection on the distance signal. For each identified time range (in red) a motion beat is selected among the candidates.  \n\nSecond, for each time range we choose as the motion beat the first candidate above $3/4$ of the interval mean.  \n\n3) Synchronization: Analysis of synchronization begins after three seconds, giving participants time to adapt to the music before scoring. We used the same synchronization scores as those defined in III-C.  \n\nC. Results  \n\nWe report in this section the results of three contrasted performances. By showing the evolution of local scores over time, Figure 5 provides a good insight of the course of the performances. In #1, the participant seemed confused during the first 5 seconds, then tapped with a regular out-of-phase pattern (off-beat syncopation) until the second 10, before being approximately in phase with the music (on the beat) for the rest of the performance. In #2, the participant was out of phase during almost all the trial duration and at times close to an offbeat syncopation. Performance #3 shows a participant globally in phase during the entire sequence, with various degrees of accuracy.  \n\nFigure 6 gives another viewpoint on these performances by showing the precise relative phase of all music/motion synchronization attempts. This representation highlights the regularity of the tapping against the music, even when it is out-of-phase. It also shows differences between participants that can globally tap before the beat (in $\\#3)$ or after it (in $\\#1)$ , as shown by the angle $\\Phi$ of the radius.  \n\nTable II summarize these trends and shows the final results of our synchronization scores (in percent) for these three performances. The local score ( $F$ -score, mean Gaussian and mean Sigmo¨ıd) shows good results for #1, better ones for #3 and very poor results for performance $\\#2$ . The Gaussian score seems to enhance good and precise performances as shown by the good result of $\\#3$ $(83\\%)$ over #1 $(73\\%)$ . In contrast the Sigmo¨ıd score penalize less the out-of-phase taps. Note that this behavior can be tuned according to the parameters of the three scores (for instance $\\sigma$ for Gaussian, and $c$ the slope of the Sigmo¨ıd).  \n\n  \nFig. 5. Estimated synchronization for three performances with music (in blue), between music beat (in red) and finger tapping (in green). Black and yellow lines represents respectively the Gaussian and Sigmoı¨d local scores.  \n\n  \nFig. 6. Polar plot of relative phases. Red circles represent each relative phase $\\phi_{i}$ from the center (start of the performance) to outer edge (end time). The angle and the size of the black radius represent $\\Phi$ and $$ .  \n\nOn the side of the global approaches, DTW show less differences between performances, in particular if we focus on #1 and #3. This confirms the experiment described in III-C with synthesized data: DTW is more sensitive to missed and added values than precise synchronization. R score reflects the regularity of the tapping phase. It is the only score which rates $\\#2$ better than #1, focusing solely on overall regularity. CSL shows more limitation in expressing a global viewpoint with a score of 0 for $\\#2$ . Indeed the continuity in this score is measured from a binary rating of each motion beat. However it provides additional information on $\\#l$ and #3, showing that #1 has the larger continuous synchronization segments, and #3 the best total number of continuous synchronization segments.  \n\nTABLE II SYNCHRONIZATION SCORE ON THREE PERFORMANCES   \n\nScores#1#2#3F-score70277Gaussian73783Sigmoid722279DTW895892R667089CSL(c -t)75-810 -048-86  \n\nOverall, the best results are achieved by performance #3, which is both locally accurate and constant over time.  \n\nV. CONCLUSION  \n\nIn this paper, we presented a tool to analyze synchronization between music and a defined motion. We evaluated a large number of scoring metrics, showing how they complement each other in highlighting different aspects of the performances. Local approaches, in particular with a continuous score as Gaussian and Sigmo¨ıd, provides valuable and precise results when the performance is globally in phase wit the music. Global approaches, that seem sometimes overlooked, provides an hortogonal viewpoint with highlighting the continuity of performance. This is crucial in the scope of music interaction, as a perfect off-beat syncopation is more valuable that a random tapping. Future work should focus on integrating all methods to generate a single score that accounts for synchronization quality, regularity, and continuity. A better comprehension and utilization of synchronization scores can help us to evaluate real-life scenarios such as dance performances, collective sports, and improve music-based gait rehabilitation devices [16].  \n\nREFERENCES  \n\n[1] Damm, L. et al. (2020). “Why do we move to the beat? A multi-scale approach, from physical principles to brain dynamics.” Neuroscience and Biobehavioral Reviews, 112, 553-584. [2] Bogdanov, D. et al. (2013). “Essentia: an open-source library for sound and music analysis.” In Proc. of ACM Multimedia. [3] McFee, B. et al. (2015). “librosa: Audio and Music Signal Analysis in Python.” In Proc. of the 14th Python in Science Conf. (SCIPY). [4] Bo¨ck, S. et al. (2016, octobre). “Madmom: A new Python audio and music signal processing library.” In Proceedings of the 24th ACM International Conference on Multimedia (pp. 1174-1178). [5] Ho, C. et al. (2013, mai). “Extraction and alignment evaluation of motion beats for street dance.” In 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (pp. 2429-2433). IEEE. [6] Menzel, M.et al (2023). “On the Beat: Analysing and Evaluating Synchronicity in Dance Performances.” [7] Faisal, I. A. et al. (2019). “A review of accelerometer sensor and gyroscope sensor in IMU sensors on motion capture.” J. Eng. Appl. Sci, 15(3), 826-829. [8] Chung, J. et al. (2022). “Comparative analysis of skeleton-based human pose estimation.” Future Internet, 14(12), 380. [9] Rose, D. et al. (2021). “A general procedure to measure the pacing of body movements timed to music and metronome in younger and older adults.” Scientific reports, 11(1), 3264.   \n[10] Moens, B. et al. (2014). “Encouraging spontaneous synchronization with D-Jogger, an adaptive music player that aligns movement and music.” PloS one, 9(12), e114234.   \n[11] Davies, M. E. P., Bo¨ck, S., and Fuentes, M. (2021, November). “Tempo, Beat and Downbeat Estimation”. Retrieved from https://tempobeatdownbeat.github.io/tutorial/intro.html   \n[12] Meinard Mu¨ller. (2007). “Dynamic Time Warping ”. (Chapter 4 of Information Retrieval for Music and Motion). Springer Verlag, ISBN: 3540740473.   \n[13] Watson, G. S. (1982). “Circular statistics in biology”.   \n[14] U. Marchand, Q. Fresnel and G. Peeters, “GTZAN-Rhythm: extending the GTZAN test-set with beat, downbeat and swing annotations.” in ISMIR 2015 Late-Breaking Session, Malaga, Spain.   \n[15] Lugaresi, C. et al. (2019). “Mediapipe: A framework for building perception pipelines.” arXiv preprint arXiv:1906.08172.   \n[16] Cochen De Cock, V., et al. (2021). “BeatWalk: Personalized music-based gait rehabilitation in Parkinson’s disease.” Front. in psychology, vol. 12.", "files_in_pdf": [{"path": ".pdf_temp/subset_1_10_41904f82_1762189311/images/gzw6iz.jpg", "size": 58376}, {"path": ".pdf_temp/subset_1_10_41904f82_1762189311/images/1256hh.jpg", "size": 60943}, {"path": ".pdf_temp/subset_1_10_41904f82_1762189311/images/8ysvcz.jpg", "size": 164893}, {"path": ".pdf_temp/subset_1_10_41904f82_1762189311/images/e09458.jpg", "size": 49214}, {"path": ".pdf_temp/subset_1_10_41904f82_1762189311/images/jqzdcs.jpg", "size": 28415}, {"path": ".pdf_temp/subset_1_10_41904f82_1762189311/images/27kkz4.jpg", "size": 20668}, {"path": ".pdf_temp/subset_1_10_41904f82_1762189311/images/9eh7ol.jpg", "size": 60008}, {"path": ".pdf_temp/subset_1_10_41904f82_1762189311/images/gtqmor.jpg", "size": 68967}]}