#!/usr/bin/env python3
"""
Executador Principal de Testes - AiShorts v2.0

Este script executa todos os testes do sistema de forma organizada:
- Testes unit√°rios (r√°pidos)
- Testes de integra√ß√£o (API key necess√°ria)
- Benchmarks de performance
- Valida√ß√£o de qualidade

Uso:
    python run_tests.py --unit          # Apenas testes unit√°rios
    python run_tests.py --integration   # Apenas testes de integra√ß√£o  
    python run_tests.py --benchmark     # Apenas benchmarks
    python run_tests.py --quality       # Apenas valida√ß√£o de qualidade
    python run_tests.py --all           # Todos os testes
    python run_tests.py --help          # Mostrar ajuda
"""

import sys
import os
import subprocess
import argparse
from pathlib import Path
from datetime import datetime

# Adicionar diret√≥rio do projeto ao path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

class TestRunner:
    """Executor principal de testes."""
    
    def __init__(self):
        self.project_root = project_root
        self.test_dir = self.project_root / "tests"
        self.results_dir = self.project_root / "data" / "test_results"
        self.results_dir.mkdir(exist_ok=True)
    
    def run_command(self, command, description):
        """Executa comando e retorna resultado."""
        print(f"\n{'='*60}")
        print(f"üîÑ {description}")
        print(f"{'='*60}")
        
        try:
            result = subprocess.run(
                command,
                cwd=self.project_root,
                capture_output=True,
                text=True,
                timeout=300  # 5 minutos timeout
            )
            
            # Exibir sa√≠da
            if result.stdout:
                print("STDOUT:")
                print(result.stdout)
            
            if result.stderr:
                print("STDERR:")
                print(result.stderr)
            
            success = result.returncode == 0
            status = "‚úÖ SUCESSO" if success else "‚ùå FALHOU"
            print(f"\n{status} - {description}")
            
            return success, result
            
        except subprocess.TimeoutExpired:
            print(f"\n‚è∞ TIMEOUT - {description}")
            return False, None
        except Exception as e:
            print(f"\nüí• ERRO - {description}: {e}")
            return False, None
    
    def run_unit_tests(self):
        """Executa testes unit√°rios."""
        print("\nüß™ EXECUTANDO TESTES UNIT√ÅRIOS")
        print("Estes testes s√£o r√°pidos e n√£o requerem API key")
        
        command = [
            "python", "-m", "pytest",
            str(self.test_dir),
            "-v",
            "-m", "unit",
            "--tb=short"
        ]
        
        return self.run_command(command, "Testes Unit√°rios")
    
    def run_integration_tests(self):
        """Executa testes de integra√ß√£o."""
        print("\nüîó EXECUTANDO TESTES DE INTEGRA√á√ÉO")
        print("Estes testes requerem OPENROUTER_API_KEY configurada")
        
        # Verificar se API key est√° configurada
        api_key = os.getenv('OPENROUTER_API_KEY')
        if not api_key:
            print("‚ö†Ô∏è  OPENROUTER_API_KEY n√£o encontrada!")
            print("   Configure a vari√°vel de ambiente ou arquivo .env")
            print("   Pulando testes de integra√ß√£o...")
            return True  # N√£o falhar por isso
        
        command = [
            "python", "-m", "pytest",
            str(self.test_dir / "test_integration.py"),
            "-v",
            "-m", "integration",
            "--tb=short"
        ]
        
        return self.run_command(command, "Testes de Integra√ß√£o")
    
    def run_benchmark_tests(self):
        """Executa testes de benchmark."""
        print("\nüìä EXECUTANDO BENCHMARKS DE PERFORMANCE")
        print("Estes testes medem performance e podem ser mais lentos")
        
        command = [
            "python", "-m", "pytest",
            str(self.test_dir / "test_benchmarks.py"),
            "-v",
            "-m", "benchmark",
            "--tb=short"
        ]
        
        return self.run_command(command, "Benchmarks de Performance")
    
    def run_quality_tests(self):
        """Executa testes de valida√ß√£o de qualidade."""
        print("\n‚≠ê EXECUTANDO VALIDA√á√ÉO DE QUALIDADE")
        print("Estes testes avaliam a qualidade dos temas gerados")
        
        command = [
            "python", "-m", "pytest",
            str(self.test_dir / "test_quality_validation.py"),
            "-v",
            "-m", "quality",
            "--tb=short"
        ]
        
        return self.run_command(command, "Valida√ß√£o de Qualidade")
    
    def run_all_tests(self):
        """Executa todos os testes."""
        print("\nüöÄ EXECUTANDO TODOS OS TESTES")
        print("Esto pode levar alguns minutos...")
        
        # Executar testes na ordem de velocidade
        results = {}
        
        # 1. Testes unit√°rios (sempre)
        results['unit'] = self.run_unit_tests()
        
        # 2. Testes de qualidade (sempre)
        results['quality'] = self.run_quality_tests()
        
        # 3. Benchmarks (opcional, se n√£o demorar muito)
        print("\nüí° Executando benchmarks b√°sicos...")
        results['benchmark'] = self.run_benchmark_tests()
        
        # 4. Testes de integra√ß√£o (opcional, se API key dispon√≠vel)
        print("\nüí° Verificando API key para testes de integra√ß√£o...")
        results['integration'] = self.run_integration_tests()
        
        return results
    
    def generate_report(self, results):
        """Gera relat√≥rio dos resultados."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = self.results_dir / f"test_report_{timestamp}.txt"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("RELAT√ìRIO DE TESTES - AiShorts v2.0\n")
            f.write("=" * 80 + "\n")
            f.write(f"Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Diret√≥rio: {self.project_root}\n\n")
            
            f.write("RESUMO DOS RESULTADOS:\n")
            f.write("-" * 40 + "\n")
            
            total_passed = 0
            total_tests = 0
            
            for test_name, (success, _) in results.items():
                status = "‚úÖ PASSOU" if success else "‚ùå FALHOU"
                f.write(f"{test_name:15} - {status}\n")
                total_tests += 1
                if success:
                    total_passed += 1
            
            f.write(f"\nRESULTADO GERAL: {total_passed}/{total_tests} grupos passaram\n")
            
            if total_passed == total_tests:
                f.write("\nüéâ TODOS OS TESTES PASSARAM!\n")
                f.write("Sistema pronto para produ√ß√£o.\n")
            else:
                f.write(f"\n‚ö†Ô∏è  {total_tests - total_passed} grupo(s) de teste falharam.\n")
                f.write("Revisar erros antes de prosseguir.\n")
            
            f.write("\n" + "=" * 80 + "\n")
        
        print(f"\nüìä Relat√≥rio salvo em: {report_file}")
        return report_file
    
    def check_dependencies(self):
        """Verifica depend√™ncias dos testes."""
        print("üîç Verificando depend√™ncias...")
        
        # Verificar pytest
        try:
            import pytest
            print("‚úÖ pytest dispon√≠vel")
        except ImportError:
            print("‚ùå pytest n√£o encontrado. Execute: pip install pytest")
            return False
        
        # Verificar psutil (para benchmarks)
        try:
            import psutil
            print("‚úÖ psutil dispon√≠vel (para benchmarks)")
        except ImportError:
            print("‚ö†Ô∏è  psutil n√£o encontrado. Execute: pip install psutil")
        
        # Verificar estrutura de arquivos
        required_files = [
            "src/core/openrouter_client.py",
            "src/generators/theme_generator.py",
            "src/generators/prompt_engineering.py",
            "tests/test_openrouter.py",
            "tests/test_theme_generator.py"
        ]
        
        for file_path in required_files:
            full_path = self.project_root / file_path
            if full_path.exists():
                print(f"‚úÖ {file_path}")
            else:
                print(f"‚ùå {file_path} - arquivo n√£o encontrado")
                return False
        
        return True
    
    def show_help(self):
        """Mostra ajuda."""
        help_text = """
üî¨ AiShorts v2.0 - Sistema de Testes

Este sistema executa testes abrangentes para validar o funcionamento
e qualidade do sistema AiShorts v2.0.

CATEGORIAS DE TESTE:

üì¶ Testes Unit√°rios (-m unit)
   - Validam componentes isoladamente
   - R√°pidos (segundos)
   - N√£o requerem API key
   
üîó Testes de Integra√ß√£o (-m integration)  
   - Testam componentes working together
   - Requerem OPENROUTER_API_KEY
   - Levam alguns minutos
   
üìä Benchmarks (-m benchmark)
   - Medem performance do sistema
   - Testam tempos de resposta
   - Verificam uso de mem√≥ria
   
‚≠ê Valida√ß√£o de Qualidade (-m quality)
   - Avaliam qualidade dos temas gerados
   - Testam m√©tricas de curiosidade/educa√ß√£o
   - Verificam consist√™ncia

OP√á√ïES:

--unit          Executa apenas testes unit√°rios
--integration   Executa apenas testes de integra√ß√£o  
--benchmark     Executa apenas benchmarks
--quality       Executa apenas valida√ß√£o de qualidade
--all           Executa todos os testes (padr√£o)
--help          Mostra esta ajuda

EXEMPLOS:

python run_tests.py --unit                    # Testes r√°pidos
python run_tests.py --all                     # Todos os testes
python run_tests.py --integration --benchmark # Integra√ß√£o + performance

CONFIGURA√á√ÉO:

Para testes de integra√ß√£o, configure:
export OPENROUTER_API_KEY="sua_chave_aqui"

Ou crie arquivo .env com:
OPENROUTER_API_KEY=sua_chave_aqui

RELAT√ìRIOS:

Resultados s√£o salvos em: data/test_results/
Arquivo: test_report_YYYYMMDD_HHMMSS.txt
        """
        print(help_text)

def main():
    """Fun√ß√£o principal."""
    parser = argparse.ArgumentParser(description="Executor de Testes AiShorts v2.0")
    parser.add_argument("--unit", action="store_true", help="Executar apenas testes unit√°rios")
    parser.add_argument("--integration", action="store_true", help="Executar apenas testes de integra√ß√£o")
    parser.add_argument("--benchmark", action="store_true", help="Executar apenas benchmarks")
    parser.add_argument("--quality", action="store_true", help="Executar apenas valida√ß√£o de qualidade")
    parser.add_argument("--all", action="store_true", help="Executar todos os testes (padr√£o)")
    parser.add_argument("--help-extended", action="store_true", help="Mostrar ajuda estendida")
    
    args = parser.parse_args()
    
    runner = TestRunner()
    
    # Mostrar ajuda estendida se solicitado
    if args.help_extended:
        runner.show_help()
        return
    
    # Verificar depend√™ncias
    if not runner.check_dependencies():
        print("\n‚ùå Depend√™ncias n√£o atendidas. Corrija os problemas acima.")
        sys.exit(1)
    
    # Determinar quais testes executar
    if not any([args.unit, args.integration, args.benchmark, args.quality]):
        args.all = True  # Padr√£o: executar todos
    
    print("üöÄ INICIANDO SISTEMA DE TESTES AiShorts v2.0")
    print(f"üìÅ Diret√≥rio do projeto: {runner.project_root}")
    print(f"üìÅ Diret√≥rio de testes: {runner.test_dir}")
    
    results = {}
    
    # Executar testes selecionados
    if args.all:
        results = runner.run_all_tests()
    else:
        if args.unit:
            results['unit'] = runner.run_unit_tests()
        if args.integration:
            results['integration'] = runner.run_integration_tests()
        if args.benchmark:
            results['benchmark'] = runner.run_benchmark_tests()
        if args.quality:
            results['quality'] = runner.run_quality_tests()
    
    # Gerar relat√≥rio
    report_file = runner.generate_report(results)
    
    # Resumo final
    total_passed = sum(1 for success, _ in results.values() if success)
    total_tests = len(results)
    
    print(f"\n{'='*60}")
    print("üèÅ EXECU√á√ÉO DE TESTES CONCLU√çDA")
    print(f"{'='*60}")
    print(f"üìä Resultados: {total_passed}/{total_tests} grupos passaram")
    
    if total_passed == total_tests:
        print("üéâ TODOS OS TESTES PASSARAM!")
        print("‚úÖ Sistema AiShorts v2.0 est√° funcionando corretamente")
    else:
        failed_groups = [name for name, (success, _) in results.items() if not success]
        print(f"‚ùå Testes falharam: {', '.join(failed_groups)}")
        print("üîß Revise os erros acima antes de prosseguir")
    
    print(f"üìÑ Relat√≥rio completo: {report_file}")
    
    # Exit code baseado no sucesso
    sys.exit(0 if total_passed == total_tests else 1)

if __name__ == "__main__":
    main()