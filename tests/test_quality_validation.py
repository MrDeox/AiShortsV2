"""
Testes de Valida√ß√£o de Qualidade - AiShorts v2.0

Testa a qualidade dos temas gerados:
- Conformidade com categorias
- Interesse/curiosidade
- Valor educacional
- Formato e estrutura
- M√©tricas de qualidade

Estes testes avaliam se os temas gerados atendem aos padr√µes
de qualidade estabelecidos para o sistema AiShorts.
"""

import pytest
from statistics import mean, median
import re

@pytest.mark.unit
class TestThemeQualityValidation:
    """Testes de valida√ß√£o de qualidade de temas."""
    
    def test_curiosity_factor_measurement(self, mock_logger):
        """Testa medi√ß√£o do fator curiosidade."""
        from src.generators.prompt_engineering import prompt_engineering, ThemeCategory
        
        print("üîç Testando medi√ß√£o de fator curiosidade...")
        
        # Temas com diferentes n√≠veis de curiosidade
        test_themes = {
            "alta_curiosidade": [
                "Por que os flamingos s√£o rosas?",
                "Como os p√°ssaros navegam usando o campo magn√©tico da Terra?",
                "Por que a neve √© branca se as mol√©culas de √°gua s√£o transparentes?",
                "Como as estrelas comuns se tornam buracos negros?",
                "Por que os gatos ronronam?"
            ],
            "media_curiosidade": [
                "Como funciona o GPS?",
                "Por que o c√©u √© azul?",
                "Como nascem as montanhas?",
                "Por que as folhas ficam amarelas no outono?",
                "Como funciona a fotoss√≠ntese?"
            ],
            "baixa_curiosidade": [
                "O c√©u √© azul.",
                "As plantas precisam de √°gua.",
                "Os cachorros s√£o animais.",
                "O fogo queima.",
                "A Terra √© redonda."
            ]
        }
        
        for category, themes in test_themes.items():
            curiosity_scores = []
            
            for theme in themes:
                metrics = prompt_engineering.get_quality_metrics(theme, ThemeCategory.SCIENCE)
                curiosity_scores.append(metrics["curiosity_factor"])
                
                print(f"   '{theme[:40]}...' = {metrics['curiosity_factor']:.2f}")
            
            avg_curiosity = mean(curiosity_scores)
            print(f"   M√©dia {category}: {avg_curiosity:.2f}")
            
            # Verificar ordena√ß√£o esperada
            if category == "alta_curiosidade":
                assert avg_curiosity > 0.7, f"Alta curiosidade deve ter score > 0.7, got {avg_curiosity}"
            elif category == "media_curiosidade":
                assert 0.4 <= avg_curiosity <= 0.8, f"M√©dia curiosidade deve estar entre 0.4-0.8, got {avg_curiosity}"
            elif category == "baixa_curiosidade":
                assert avg_curiosity < 0.6, f"Baixa curiosidade deve ter score < 0.6, got {avg_curiosity}"
    
    def test_educational_value_assessment(self, mock_logger):
        """Testa avalia√ß√£o de valor educacional."""
        from src.generators.prompt_engineering import prompt_engineering, ThemeCategory
        
        print("üéì Testando avalia√ß√£o de valor educacional...")
        
        # Temas com diferentes valores educacionais
        educational_themes = [
            "Como funciona a teoria da relatividade de Einstein?",
            "Por que os dinossauros se extinguiram?",
            "Como os antigos eg√≠pcios constru√≠ram as pir√¢mides?",
            "Por que o sal derrete o gelo?",
            "Como o sistema imunol√≥gico protege o corpo?"
        ]
        
        non_educational_themes = [
            "Qual a sua cor favorita?",
            "Voc√™ gosta de pizza?",
            "Que horas s√£o?",
            "Como est√° o tempo hoje?",
            "Voc√™ tem fome?"
        ]
        
        # Testar temas educacionais
        edu_scores = []
        for theme in educational_themes:
            metrics = prompt_engineering.get_quality_metrics(theme, ThemeCategory.SCIENCE)
            edu_scores.append(metrics["educational_value"])
            print(f"   Educacional: '{theme[:40]}...' = {metrics['educational_value']:.2f}")
        
        avg_edu = mean(edu_scores)
        print(f"   M√©dia educacional: {avg_edu:.2f}")
        
        # Testar temas n√£o-educacionais
        non_edu_scores = []
        for theme in non_educational_themes:
            metrics = prompt_engineering.get_quality_metrics(theme, ThemeCategory.SCIENCE)
            non_edu_scores.append(metrics["educational_value"])
            print(f"   N√£o-educacional: '{theme[:40]}...' = {metrics['educational_value']:.2f}")
        
        avg_non_edu = mean(non_edu_scores)
        print(f"   M√©dia n√£o-educacional: {avg_non_edu:.2f}")
        
        # Temas educacionais devem ter score mais alto
        assert avg_edu > avg_non_edu, f"Temas educacionais devem ter maior valor educacional"
        assert avg_edu > 0.6, f"Temas educacionais devem ter score > 0.6, got {avg_edu}"
        assert avg_non_edu < 0.5, f"Temas n√£o-educacionais devem ter score < 0.5, got {avg_non_edu}"
    
    def test_overall_quality_calculation(self, test_utils, mock_logger):
        """Testa c√°lculo de qualidade geral."""
        from src.generators.prompt_engineering import prompt_engineering, ThemeCategory
        
        print("‚≠ê Testando c√°lculo de qualidade geral...")
        
        # Temas de diferentes qualidades
        quality_examples = {
            "excelente": "Por que os flamingos s√£o rosas? Uma curiosidade fascinante sobre sua dieta rica em carotenoides!",
            "boa": "Por que o c√©u √© azul?",
            "regular": "Como funciona o GPS de forma simples?",
            "ruim": "Azul.",
            "pessima": "?"  # Extremamente ruim
        }
        
        for quality_level, theme in quality_examples.items():
            metrics = prompt_engineering.get_quality_metrics(theme, ThemeCategory.SCIENCE)
            overall_score = metrics["overall_quality"]
            
            print(f"   {quality_level}: {overall_score:.2f}")
            
            # Verificar ordena√ß√£o esperada
            if quality_level == "excelente":
                assert overall_score > 0.8, f"Excelente deve ter score > 0.8, got {overall_score}"
            elif quality_level == "boa":
                assert 0.6 < overall_score <= 0.8, f"Boa deve ter score 0.6-0.8, got {overall_score}"
            elif quality_level == "regular":
                assert 0.4 <= overall_score <= 0.6, f"Regular deve ter score 0.4-0.6, got {overall_score}"
            elif quality_level == "ruim":
                assert 0.1 <= overall_score < 0.4, f"Ruim deve ter score 0.1-0.4, got {overall_score}"
            elif quality_level == "pessima":
                assert overall_score < 0.2, f"P√©ssima deve ter score < 0.2, got {overall_score}"
    
    def test_category_specific_quality(self, mock_logger):
        """Testa qualidade espec√≠fica por categoria."""
        from src.generators.prompt_engineering import prompt_engineering, ThemeCategory
        
        print("üè∑Ô∏è Testando qualidade espec√≠fica por categoria...")
        
        # Temas espec√≠ficos para cada categoria
        category_themes = {
            ThemeCategory.SPACE: "Por que os planetas s√£o redondos?",
            ThemeCategory.ANIMALS: "Como os golfinhos dormem?",
            ThemeCategory.PSYCHOLOGY: "Por que temos D√©j√† vu?",
            ThemeCategory.GEOGRAPHY: "Por que os vulc√µes entram em erup√ß√£o?",
            ThemeCategory.FOOD: "Por que o chocolate derrete na boca?"
        }
        
        category_scores = {}
        
        for category, theme in category_themes.items():
            metrics = prompt_engineering.get_quality_metrics(theme, category)
            overall_score = metrics["overall_quality"]
            
            category_scores[category.value] = overall_score
            print(f"   {category.value}: {overall_score:.2f}")
            
            # Cada categoria deve ter qualidade m√≠nima
            assert overall_score > 0.5, f"Categoria {category.value} tem qualidade muito baixa: {overall_score}"
        
        # Verificar se todas as categorias t√™m qualidade similar (n√£o muito diferente)
        scores = list(category_scores.values())
        min_score = min(scores)
        max_score = max(scores)
        score_range = max_score - min_score
        
        print(f"   Range de qualidade: {score_range:.2f}")
        
        # Range n√£o deve ser muito grande (menos de 0.3)
        assert score_range < 0.3, f"Qualidade muito inconsistente entre categorias: {score_range}"

@pytest.mark.unit
class TestThemeContentValidation:
    """Testes de valida√ß√£o de conte√∫do de temas."""
    
    def test_question_format_validation(self, mock_logger):
        """Testa valida√ß√£o de formato de pergunta."""
        from src.generators.prompt_engineering import prompt_engineering
        
        print("‚ùì Testando valida√ß√£o de formato de pergunta...")
        
        # Formatos v√°lidos de pergunta
        valid_questions = [
            "Por que o c√©u √© azul?",
            "Como funciona o GPS?",
            "Qual a origem dos dinossauros?",
            "Quando foram inventadas as rodas?",
            "Onde nascem os rios?",
            "Quem descobriu a penicilina?",
            "Por que os flamingos s√£o rosas?"
        ]
        
        for question in valid_questions:
            is_valid = prompt_engineering.validate_prompt_format(question)
            print(f"   ‚úì '{question}' = {is_valid}")
            assert is_valid is True, f"Pergunta v√°lida foi rejeitada: {question}"
        
        # Formatos inv√°lidos
        invalid_questions = [
            "",  # Vazio
            "   ",  # Apenas espa√ßos
            "O c√©u √© azul",  # Afirmativa
            "Azul",  # Muito curto
            "Sabe me dizer por que o c√©u √© azul?",  # Iniciando com "sabe"
            "Pode me explicar como funciona?",  # Vago
            "Sobre o c√©u azul, o que voc√™ pode dizer?"  # N√£o √© pergunta direta
        ]
        
        for question in invalid_questions:
            is_valid = prompt_engineering.validate_prompt_format(question)
            print(f"   ‚úó '{question}' = {is_valid}")
            assert is_valid is False, f"Pergunta inv√°lida foi aceita: {question}"
    
    def test_content_appropriateness(self, mock_logger):
        """Testa apropria√ß√£o de conte√∫do."""
        from src.generators.prompt_engineering import prompt_engineering, ThemeCategory
        
        print("üìù Testando apropria√ß√£o de conte√∫do...")
        
        # Conte√∫dos apropriados por categoria
        appropriate_content = {
            ThemeCategory.SCIENCE: [
                "Por que o gelo flutua na √°gua?",
                "Como funciona a fotoss√≠ntese?",
                "Por que vemos arco-√≠ris?"
            ],
            ThemeCategory.HISTORY: [
                "Como funcionava o calend√°rio eg√≠pcio?",
                "Quem foram os fara√≥s do Egito?",
                "Como eram constru√≠das as pir√¢mides?"
            ],
            ThemeCategory.SPACE: [
                "Por que os planetas giram?",
                "Como nascem as estrelas?",
                "Existe vida em outros planetas?"
            ]
        }
        
        for category, themes in appropriate_content.items():
            for theme in themes:
                # N√£o deve levantar exce√ß√£o
                metrics = prompt_engineering.get_quality_metrics(theme, category)
                assert metrics is not None
                assert metrics["overall_quality"] > 0
        
        # Conte√∫dos inapropriados devem ter baixa qualidade
        inappropriate_content = [
            "Como fazer bombas?",
            "Qual a melhor forma de roubar?",
            "Como enganar as pessoas?",
            "Qual a receita de drogas?"
        ]
        
        for theme in inappropriate_content:
            metrics = prompt_engineering.get_quality_metrics(theme, ThemeCategory.SCIENCE)
            # Deve ter qualidade muito baixa
            assert metrics["overall_quality"] < 0.3, f"Conte√∫do inapropriado teve qualidade alta: {theme}"
            print(f"   Inapropriado rejeitado: '{theme}' = {metrics['overall_quality']:.2f}")
    
    def test_language_and_grammar(self, mock_logger):
        """Testa valida√ß√£o de idioma e gram√°tica."""
        from src.generators.prompt_engineering import prompt_engineering
        
        print("üìö Testando idioma e gram√°tica...")
        
        # Temas em portugu√™s com gram√°tica correta
        valid_portuguese = [
            "Por que o c√©u √© azul?",
            "Como funcionam os neutrinos?",
            "Qual a origem da vida na Terra?",
            "Por que os p√°ssaros migram?",
            "Como os diamonds s√£o formados?"
        ]
        
        for theme in valid_portuguese:
            is_valid = prompt_engineering.validate_prompt_format(theme)
            assert is_valid is True, f"Tema portugu√™s v√°lido rejeitado: {theme}"
        
        # Temas com problemas gramaticais
        invalid_grammar = [
            "Por que o c√©u azul √©?",  # Ordem incorreta
            "Como funciona o ?",  # Incompleto
            "Qual o origem da vida",  # Faltando artigo
            "Por que as plantas precisa de luz",  # Concord√¢ncia incorreta
            "Como os flamingos s√£o rosas s√£o?"  # Repeti√ß√£o
        ]
        
        for theme in invalid_grammar:
            is_valid = prompt_engine_engineering.validate_prompt_format(theme)
            # Alguns podem passar, mas devem ter qualidade baixa
            if is_valid:
                metrics = prompt_engineering.get_quality_metrics(theme, ThemeCategory.SCIENCE)
                assert metrics["overall_quality"] < 0.7, f"Gram√°tica incorreta teve alta qualidade: {theme}"

@pytest.mark.unit
class TestQualityMetricsConsistency:
    """Testes de consist√™ncia das m√©tricas de qualidade."""
    
    def test_metrics_correlation(self, mock_logger):
        """Testa correla√ß√£o entre m√©tricas."""
        from src.generators.prompt_engineering import prompt_engineering, ThemeCategory
        
        print("üìä Testando correla√ß√£o entre m√©tricas...")
        
        test_themes = [
            "Por que os flamingos s√£o rosas?",
            "Como funciona a relatividade?",
            "Por que o oceano √© salgado?",
            "Azul.",
            "?", 
            "Qual sua cor favorita?",
            "Como nascem as estrelas?",
            "Voc√™ gosta de pizza?"
        ]
        
        all_metrics = []
        for theme in test_themes:
            metrics = prompt_engineering.get_quality_metrics(theme, ThemeCategory.SCIENCE)
            all_metrics.append(metrics)
            print(f"   Tema: '{theme[:30]}...'")
            print(f"     Curiosidade: {metrics['curiosity_factor']:.2f}")
            print(f"     Educacional: {metrics['educational_value']:.2f}")
            print(f"     Geral: {metrics['overall_quality']:.2f}")
        
        # Verificar se m√©tricas gerais est√£o correlacionadas
        curiosity_scores = [m["curiosity_factor"] for m in all_metrics]
        educational_scores = [m["educational_value"] for m in all_metrics]
        overall_scores = [m["overall_quality"] for m in all_metrics]
        
        # Qualidade geral deve ser uma combina√ß√£o das outras
        for i, (cur, edu, overall) in enumerate(zip(curiosity_scores, educational_scores, overall_scores)):
            # Qualidade geral deve estar entre as outras m√©tricas (aproximadamente)
            min_component = min(cur, edu)
            max_component = max(cur, edu)
            if not (min_component <= overall <= max_component or abs(overall - min_component) < 0.3):
                print(f"   ‚ö†Ô∏è Inconsist√™ncia na posi√ß√£o {i}: overall={overall:.2f}, componentes={cur:.2f},{edu:.2f}")
    
    def test_metrics_bounds(self, mock_logger):
        """Testa se m√©tricas est√£o dentro dos limites esperados."""
        from src.generators.prompt_engineering import prompt_engineering, ThemeCategory
        
        print("üìè Testando limites das m√©tricas...")
        
        # Testar com temas extremos
        extreme_themes = [
            "?",  # M√≠nimo absoluto
            "Por que os flamingos s√£o rosas? Uma quest√£o fascinante sobre carotenoides e bioqu√≠mica!",  # M√°ximo
            "O c√©u √© azul",  # Baixo
            "",  # Vazio
            "Como funciona a relatividade geral de Einstein de forma extremamente detalhada e cientificamente precisa?"  # Alto
        ]
        
        for theme in extreme_themes:
            metrics = prompt_engineering.get_quality_metrics(theme, ThemeCategory.SCIENCE)
            
            for metric_name, value in metrics.items():
                # Todas as m√©tricas devem estar entre 0 e 1
                assert 0 <= value <= 1, f"M√©trica {metric_name} fora dos limites: {value}"
                
                # Evitar valores exatamente 0 ou 1 (muito extremistas)
                if metric_name == "overall_quality":
                    if theme in ["?", ""]:  # Temas ruins
                        assert value <= 0.2, f"Tema ruim teve alta qualidade: {theme} = {value}"
                    elif "flamingos" in theme.lower():  # Tema bom
                        assert value >= 0.7, f"Tema bom teve baixa qualidade: {theme} = {value}"
            
            print(f"   ‚úì '{theme[:30]}...' - Todas m√©tricas nos limites")
    
    def test_reproducibility(self, mock_logger):
        """Testa reprodutibilidade das m√©tricas."""
        from src.generators.prompt_engineering import prompt_engineering, ThemeCategory
        
        print("üîÑ Testando reprodutibilidade das m√©tricas...")
        
        test_theme = "Por que o c√©u √© azul?"
        
        # Calcular m√©tricas m√∫ltiplas vezes
        metrics_runs = []
        for i in range(3):
            metrics = prompt_engineering.get_quality_metrics(test_theme, ThemeCategory.SCIENCE)
            metrics_runs.append(metrics)
            time.sleep(0.1)  # Pequena pausa
        
        # Verificar se todas as execu√ß√µes deram resultados similares
        curiosity_scores = [m["curiosity_factor"] for m in metrics_runs]
        educational_scores = [m["educational_value"] for m in metrics_runs]
        overall_scores = [m["overall_quality"] for m in metrics_runs]
        
        # Calcular varia√ß√£o
        curiosity_variance = max(curiosity_scores) - min(curiosity_scores)
        educational_variance = max(educational_scores) - min(educational_scores)
        overall_variance = max(overall_scores) - min(overall_scores)
        
        print(f"   Varia√ß√£o curiosidade: {curiosity_variance:.3f}")
        print(f"   Varia√ß√£o educacional: {educational_variance:.3f}")
        print(f"   Varia√ß√£o geral: {overall_variance:.3f}")
        
        # Varia√ß√£o deve ser pequena (menos de 0.1)
        assert curiosity_variance < 0.1, f"Varia√ß√£o muito alta na curiosidade: {curiosity_variance}"
        assert educational_variance < 0.1, f"Varia√ß√£o muito alta no educacional: {educational_variance}"
        assert overall_variance < 0.1, f"Varia√ß√£o muito alta no geral: {overall_variance}"
        
        print("‚úÖ M√©tricas s√£o reprodut√≠veis")

@pytest.mark.unit
class TestQualityThresholds:
    """Testes de thresholds de qualidade."""
    
    def test_quality_threshold_validation(self, mock_logger):
        """Testa valida√ß√£o com diferentes thresholds."""
        from src.generators.theme_generator import theme_generator
        from src.generators.prompt_engineering import ThemeCategory
        
        print("‚öñÔ∏è Testando thresholds de qualidade...")
        
        # Temas de exemplo com qualidades conhecidas
        test_theme = "Por que os flamingos s√£o rosas?"
        
        # Verificar m√©tricas b√°sicas
        from src.generators.prompt_engineering import prompt_engineering
        metrics = prompt_engineering.get_quality_metrics(test_theme, ThemeCategory.SCIENCE)
        actual_quality = metrics["overall_quality"]
        
        print(f"   Qualidade do tema teste: {actual_quality:.2f}")
        
        # Testar diferentes thresholds
        thresholds = [0.3, 0.5, 0.7, 0.9]
        
        for threshold in thresholds:
            meets_threshold = actual_quality >= threshold
            print(f"   Threshold {threshold}: {'PASSOU' if meets_threshold else 'FALHOU'}")
            
            # Se qualidade conhecida for alta, deve passar thresholds baixos
            if actual_quality > 0.7:
                assert meets_threshold or threshold > 0.8, f"Qualidade alta ({actual_quality}) falhou threshold baixo ({threshold})"
    
    def test_realistic_quality_distribution(self, mock_logger):
        """Testa distribui√ß√£o realista de qualidades."""
        from src.generators.prompt_engineering import prompt_engineering, ThemeCategory
        
        print("üìà Testando distribui√ß√£o realista de qualidades...")
        
        # Amostra de temas reais
        real_themes = [
            "Por que os flamingos s√£o rosas?",
            "Como funciona o GPS?",
            "Por que o c√©u √© azul?",
            "Como nascem as estrelas?",
            "Por que os p√°ssaros n√£o caem do c√©u quando dormem?",
            "Como os diamantes s√£o formados?",
            "Por que as plantas s√£o verdes?",
            "Como funciona a velocidade da luz?"
        ]
        
        qualities = []
        for theme in real_themes:
            metrics = prompt_engineering.get_quality_metrics(theme, ThemeCategory.SCIENCE)
            quality = metrics["overall_quality"]
            qualities.append(quality)
            print(f"   '{theme[:40]}...' = {quality:.2f}")
        
        # An√°lise estat√≠stica
        avg_quality = mean(qualities)
        min_quality = min(qualities)
        max_quality = max(qualities)
        
        print(f"   Estat√≠sticas:")
        print(f"     M√©dia: {avg_quality:.2f}")
        print(f"     M√≠nimo: {min_quality:.2f}")
        print(f"     M√°ximo: {max_quality:.2f}")
        
        # Verifica√ß√µes de sanidade
        assert avg_quality > 0.5, f"M√©dia muito baixa: {avg_quality}"
        assert min_quality > 0.2, f"Qualidade m√≠nima muito baixa: {min_quality}"
        assert max_quality < 1.0, f"Qualidade m√°xima muito alta: {max_quality}"
        assert max_quality - min_quality > 0.3, "Pouca varia√ß√£o nas qualidades"

# Marcador personalizado para testes de qualidade
def pytest_configure(config):
    """Adiciona marcador para testes de qualidade."""
    config.addinivalue_line("markers", "quality: marca testes de valida√ß√£o de qualidade")

if __name__ == "__main__":
    # Executar apenas testes de qualidade
    pytest.main([__file__, "-v", "-m", "quality"])